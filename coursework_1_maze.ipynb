{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "20IyxDzgp3tU"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import random\n",
        "import matplotlib.pyplot as plt # Graphical library\n",
        "#from sklearn.metrics import mean_squared_error # Mean-squared error function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2Fr69C0UBQk"
      },
      "source": [
        "# Coursework 1 :\n",
        "See pdf for instructions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QsKvVllvvreH"
      },
      "outputs": [],
      "source": [
        "# WARNING: fill in these two functions that will be used by the auto-marking script\n",
        "# [Action required]\n",
        "\n",
        "def get_CID():\n",
        "  return \"01737344\" # Return your CID (add 0 at the beginning to ensure it is 8 digits long)\n",
        "\n",
        "def get_login():\n",
        "  return \"af723\" # Return your short imperial login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKEz3d9NUbdO"
      },
      "source": [
        "## Helper class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZWnMW3GNpjd7"
      },
      "outputs": [],
      "source": [
        "# This class is used ONLY for graphics\n",
        "# YOU DO NOT NEED to understand it to work on this coursework\n",
        "\n",
        "class GraphicsMaze(object):\n",
        "\n",
        "  def __init__(self, shape, locations, default_reward, obstacle_locs, absorbing_locs, absorbing_rewards, absorbing):\n",
        "\n",
        "    self.shape = shape\n",
        "    self.locations = locations\n",
        "    self.absorbing = absorbing\n",
        "\n",
        "    # Walls\n",
        "    self.walls = np.zeros(self.shape)\n",
        "    for ob in obstacle_locs:\n",
        "      self.walls[ob] = 20\n",
        "\n",
        "    # Rewards\n",
        "    self.rewarders = np.ones(self.shape) * default_reward\n",
        "    for i, rew in enumerate(absorbing_locs):\n",
        "      self.rewarders[rew] = 10 if absorbing_rewards[i] > 0 else -10\n",
        "\n",
        "    # Print the map to show it\n",
        "    self.paint_maps()\n",
        "\n",
        "  def paint_maps(self):\n",
        "    \"\"\"\n",
        "    Print the Maze topology (obstacles, absorbing states and rewards)\n",
        "    input: /\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.imshow(self.walls + self.rewarders)\n",
        "    plt.show()\n",
        "\n",
        "  def paint_state(self, state):\n",
        "    \"\"\"\n",
        "    Print one state on the Maze topology (obstacles, absorbing states and rewards)\n",
        "    input: /\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    states = np.zeros(self.shape)\n",
        "    states[state] = 30\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.imshow(self.walls + self.rewarders + states)\n",
        "    plt.show()\n",
        "\n",
        "  def draw_deterministic_policy(self, Policy):\n",
        "    \"\"\"\n",
        "    Draw a deterministic policy\n",
        "    input: Policy {np.array} -- policy to draw (should be an array of values between 0 and 3 (actions))\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze\n",
        "    for state, action in enumerate(Policy):\n",
        "      if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
        "        continue\n",
        "      arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
        "      action_arrow = arrows[action] # Take the corresponding action\n",
        "      location = self.locations[state] # Compute its location on graph\n",
        "      plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
        "    plt.show()\n",
        "\n",
        "  def draw_policy(self, Policy):\n",
        "    \"\"\"\n",
        "    Draw a policy (draw an arrow in the most probable direction)\n",
        "    input: Policy {np.array} -- policy to draw as probability\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    deterministic_policy = np.array([np.argmax(Policy[row,:]) for row in range(Policy.shape[0])])\n",
        "    self.draw_deterministic_policy(deterministic_policy)\n",
        "\n",
        "  def draw_value(self, Value):\n",
        "    \"\"\"\n",
        "    Draw a policy value\n",
        "    input: Value {np.array} -- policy values to draw\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze\n",
        "    for state, value in enumerate(Value):\n",
        "      if(self.absorbing[0, state]): # If it is an absorbing state, don't plot any value\n",
        "        continue\n",
        "      location = self.locations[state] # Compute the value location on graph\n",
        "      plt.text(location[1], location[0], round(value,2), ha='center', va='center') # Place it on graph\n",
        "    plt.show()\n",
        "\n",
        "  def draw_deterministic_policy_grid(self, Policies, title, n_columns, n_lines):\n",
        "    \"\"\"\n",
        "    Draw a grid representing multiple deterministic policies\n",
        "    input: Policies {np.array of np.array} -- array of policies to draw (each should be an array of values between 0 and 3 (actions))\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for subplot in range (len(Policies)): # Go through all policies\n",
        "      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each policy\n",
        "      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze\n",
        "      for state, action in enumerate(Policies[subplot]):\n",
        "        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
        "          continue\n",
        "        arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
        "        action_arrow = arrows[action] # Take the corresponding action\n",
        "        location = self.locations[state] # Compute its location on graph\n",
        "        plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
        "      ax.title.set_text(title[subplot]) # Set the title for the graph given as argument\n",
        "    plt.show()\n",
        "\n",
        "  def draw_policy_grid(self, Policies, title, n_columns, n_lines):\n",
        "    \"\"\"\n",
        "    Draw a grid representing multiple policies (draw an arrow in the most probable direction)\n",
        "    input: Policy {np.array} -- array of policies to draw as probability\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    deterministic_policies = np.array([[np.argmax(Policy[row,:]) for row in range(Policy.shape[0])] for Policy in Policies])\n",
        "    self.draw_deterministic_policy_grid(deterministic_policies, title, n_columns, n_lines)\n",
        "\n",
        "  def draw_value_grid(self, Values, title, n_columns, n_lines):\n",
        "    \"\"\"\n",
        "    Draw a grid representing multiple policy values\n",
        "    input: Values {np.array of np.array} -- array of policy values to draw\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for subplot in range (len(Values)): # Go through all values\n",
        "      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each value\n",
        "      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze\n",
        "      for state, value in enumerate(Values[subplot]):\n",
        "        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any value\n",
        "          continue\n",
        "        location = self.locations[state] # Compute the value location on graph\n",
        "        plt.text(location[1], location[0], round(value,1), ha='center', va='center') # Place it on graph\n",
        "      ax.title.set_text(title[subplot]) # Set the title for the graoh given as argument\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbY8DCqoVJlw"
      },
      "source": [
        "## Maze class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MXc1OFvZqJfZ"
      },
      "outputs": [],
      "source": [
        "# This class define the Maze environment\n",
        "\n",
        "class Maze(object):\n",
        "\n",
        "  # [Action required]\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Maze initialisation.\n",
        "    input: /\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    y = int(get_CID()[-2])\n",
        "    z = int(get_CID()[-1])\n",
        "    # [Action required]\n",
        "    # Properties set from the CID\n",
        "    self._prob_success = 0.8 + 0.02 * (9 - y) # float\n",
        "    self._gamma = 0.8 + 0.02 * y # float\n",
        "    self._goal = z % 4 # integer (0 for R0, 1 for R1, 2 for R2, 3 for R3)\n",
        "\n",
        "    # Build the maze\n",
        "    self._build_maze()\n",
        "                              \n",
        "\n",
        "  # Functions used to build the Maze environment \n",
        "  # You DO NOT NEED to modify them\n",
        "  def _build_maze(self):\n",
        "    \"\"\"\n",
        "    Maze initialisation.\n",
        "    input: /\n",
        "    output: /\n",
        "    \"\"\"\n",
        "\n",
        "    # Properties of the maze\n",
        "    self._shape = (13, 10)\n",
        "    self._obstacle_locs = [\n",
        "                          (1,0), (1,1), (1,2), (1,3), (1,4), (1,7), (1,8), (1,9), \\\n",
        "                          (2,1), (2,2), (2,3), (2,7), \\\n",
        "                          (3,1), (3,2), (3,3), (3,7), \\\n",
        "                          (4,1), (4,7), \\\n",
        "                          (5,1), (5,7), \\\n",
        "                          (6,5), (6,6), (6,7), \\\n",
        "                          (8,0), \\\n",
        "                          (9,0), (9,1), (9,2), (9,6), (9,7), (9,8), (9,9), \\\n",
        "                          (10,0)\n",
        "                         ] # Location of obstacles\n",
        "    self._absorbing_locs = [(2,0), (2,9), (10,1), (12,9)] # Location of absorbing states\n",
        "    self._absorbing_rewards = [ (500 if (i == self._goal) else -50) for i in range (4) ]\n",
        "    self._starting_locs = [(0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8), (0,9)] #Reward of absorbing states\n",
        "    self._default_reward = -1 # Reward for each action performs in the environment\n",
        "    self._max_t = 500 # Max number of steps in the environment\n",
        "\n",
        "    # Actions\n",
        "    self._action_size = 4\n",
        "    self._direction_names = ['N','E','S','W'] # Direction 0 is 'N', 1 is 'E' and so on\n",
        "        \n",
        "    # States\n",
        "    self._locations = []\n",
        "    for i in range (self._shape[0]):\n",
        "      for j in range (self._shape[1]):\n",
        "        loc = (i,j) \n",
        "        # Adding the state to locations if it is no obstacle\n",
        "        if self._is_location(loc):\n",
        "          self._locations.append(loc)\n",
        "    self._state_size = len(self._locations)\n",
        "\n",
        "    # Neighbours - each line is a state, ranked by state-number, each column is a direction (N, E, S, W)\n",
        "    self._neighbours = np.zeros((self._state_size, 4)) \n",
        "    \n",
        "    for state in range(self._state_size):\n",
        "      loc = self._get_loc_from_state(state)\n",
        "\n",
        "      # North\n",
        "      neighbour = (loc[0]-1, loc[1]) # North neighbours location\n",
        "      if self._is_location(neighbour):\n",
        "        self._neighbours[state][self._direction_names.index('N')] = self._get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        self._neighbours[state][self._direction_names.index('N')] = state\n",
        "\n",
        "      # East\n",
        "      neighbour = (loc[0], loc[1]+1) # East neighbours location\n",
        "      if self._is_location(neighbour):\n",
        "        self._neighbours[state][self._direction_names.index('E')] = self._get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        self._neighbours[state][self._direction_names.index('E')] = state\n",
        "\n",
        "      # South\n",
        "      neighbour = (loc[0]+1, loc[1]) # South neighbours location\n",
        "      if self._is_location(neighbour):\n",
        "        self._neighbours[state][self._direction_names.index('S')] = self._get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        self._neighbours[state][self._direction_names.index('S')] = state\n",
        "\n",
        "      # West\n",
        "      neighbour = (loc[0], loc[1]-1) # West neighbours location\n",
        "      if self._is_location(neighbour):\n",
        "        self._neighbours[state][self._direction_names.index('W')] = self._get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        self._neighbours[state][self._direction_names.index('W')] = state\n",
        "\n",
        "    # Absorbing\n",
        "    self._absorbing = np.zeros((1, self._state_size))\n",
        "    for a in self._absorbing_locs:\n",
        "      absorbing_state = self._get_state_from_loc(a)\n",
        "      self._absorbing[0, absorbing_state] = 1\n",
        "\n",
        "    # Transition matrix\n",
        "    self._T = np.zeros((self._state_size, self._state_size, self._action_size)) # Empty matrix of domension S*S*A\n",
        "    for action in range(self._action_size):\n",
        "      for outcome in range(4): # For each direction (N, E, S, W)\n",
        "        # The agent has prob_success probability to go in the correct direction\n",
        "        if action == outcome:\n",
        "          prob = 1 - 3.0 * ((1.0 - self._prob_success) / 3.0) # (theoritically equal to self.prob_success but avoid rounding error and garanty a sum of 1)\n",
        "        # Equal probability to go into one of the other directions\n",
        "        else:\n",
        "          prob = (1.0 - self._prob_success) / 3.0\n",
        "          \n",
        "        # Write this probability in the transition matrix\n",
        "        for prior_state in range(self._state_size):\n",
        "          # If absorbing state, probability of 0 to go to any other states\n",
        "          if not self._absorbing[0, prior_state]:\n",
        "            post_state = self._neighbours[prior_state, outcome] # Post state number\n",
        "            post_state = int(post_state) # Transform in integer to avoid error\n",
        "            self._T[prior_state, post_state, action] += prob\n",
        "\n",
        "    # Reward matrix\n",
        "    self._R = np.ones((self._state_size, self._state_size, self._action_size)) # Matrix filled with 1\n",
        "    self._R = self._default_reward * self._R # Set default_reward everywhere\n",
        "    for i in range(len(self._absorbing_rewards)): # Set absorbing states rewards\n",
        "      post_state = self._get_state_from_loc(self._absorbing_locs[i])\n",
        "      self._R[:,post_state,:] = self._absorbing_rewards[i]\n",
        "\n",
        "    # Creating the graphical Maze world\n",
        "    self._graphics = GraphicsMaze(self._shape, self._locations, self._default_reward, self._obstacle_locs, self._absorbing_locs, self._absorbing_rewards, self._absorbing)\n",
        "    \n",
        "    # Reset the environment\n",
        "    self.reset()\n",
        "\n",
        "\n",
        "  def _is_location(self, loc):\n",
        "    \"\"\"\n",
        "    Is the location a valid state (not out of Maze and not an obstacle)\n",
        "    input: loc {tuple} -- location of the state\n",
        "    output: _ {bool} -- is the location a valid state\n",
        "    \"\"\"\n",
        "    if (loc[0] < 0 or loc[1] < 0 or loc[0] > self._shape[0]-1 or loc[1] > self._shape[1]-1):\n",
        "      return False\n",
        "    elif (loc in self._obstacle_locs):\n",
        "      return False\n",
        "    else:\n",
        "      return True\n",
        "\n",
        "\n",
        "  def _get_state_from_loc(self, loc):\n",
        "    \"\"\"\n",
        "    Get the state number corresponding to a given location\n",
        "    input: loc {tuple} -- location of the state\n",
        "    output: index {int} -- corresponding state number\n",
        "    \"\"\"\n",
        "    return self._locations.index(tuple(loc))\n",
        "\n",
        "\n",
        "  def _get_loc_from_state(self, state):\n",
        "    \"\"\"\n",
        "    Get the state number corresponding to a given location\n",
        "    input: index {int} -- state number\n",
        "    output: loc {tuple} -- corresponding location\n",
        "    \"\"\"\n",
        "    return self._locations[state]\n",
        "\n",
        "  # Getter functions used only for DP agents\n",
        "  # You DO NOT NEED to modify them\n",
        "  def get_T(self):\n",
        "    return self._T\n",
        "\n",
        "  def get_R(self):\n",
        "    return self._R\n",
        "\n",
        "  def get_absorbing(self):\n",
        "    return self._absorbing\n",
        "\n",
        "  # Getter functions used for DP, MC and TD agents\n",
        "  # You DO NOT NEED to modify them\n",
        "  def get_graphics(self):\n",
        "    return self._graphics\n",
        "\n",
        "  def get_action_size(self):\n",
        "    return self._action_size\n",
        "\n",
        "  def get_state_size(self):\n",
        "    return self._state_size\n",
        "\n",
        "  def get_gamma(self):\n",
        "    return self._gamma\n",
        "\n",
        "  # Functions used to perform episodes in the Maze environment\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment state to one of the possible starting states\n",
        "    input: /\n",
        "    output: \n",
        "      - t {int} -- current timestep\n",
        "      - state {int} -- current state of the envionment\n",
        "      - reward {int} -- current reward\n",
        "      - done {bool} -- True if reach a terminal state / 0 otherwise\n",
        "    \"\"\"\n",
        "    self._t = 0\n",
        "    self._state = self._get_state_from_loc(self._starting_locs[random.randrange(len(self._starting_locs))])\n",
        "    self._reward = 0\n",
        "    self._done = False\n",
        "    return self._t, self._state, self._reward, self._done\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Perform an action in the environment\n",
        "    input: action {int} -- action to perform\n",
        "    output: \n",
        "      - t {int} -- current timestep\n",
        "      - state {int} -- current state of the envionment\n",
        "      - reward {int} -- current reward\n",
        "      - done {bool} -- True if reach a terminal state / 0 otherwise\n",
        "    \"\"\"\n",
        "\n",
        "    # If environment already finished, print an error\n",
        "    if self._done or self._absorbing[0, self._state]:\n",
        "      print(\"Please reset the environment\")\n",
        "      return self._t, self._state, self._reward, self._done\n",
        "\n",
        "    # Drawing a random number used for probaility of next state\n",
        "    probability_success = random.uniform(0,1)\n",
        "\n",
        "    # Look for the first possible next states (so get a reachable state even if probability_success = 0)\n",
        "    new_state = 0\n",
        "    while self._T[self._state, new_state, action] == 0: \n",
        "      new_state += 1\n",
        "    assert self._T[self._state, new_state, action] != 0, \"Selected initial state should be probability 0, something might be wrong in the environment.\"\n",
        "\n",
        "    # Find the first state for which probability of occurence matches the random value\n",
        "    total_probability = self._T[self._state, new_state, action]\n",
        "    while (total_probability < probability_success) and (new_state < self._state_size-1):\n",
        "     new_state += 1\n",
        "     total_probability += self._T[self._state, new_state, action]\n",
        "    assert self._T[self._state, new_state, action] != 0, \"Selected state should be probability 0, something might be wrong in the environment.\"\n",
        "    \n",
        "    # Setting new t, state, reward and done\n",
        "    self._t += 1\n",
        "    self._reward = self._R[self._state, new_state, action]\n",
        "    self._done = self._absorbing[0, new_state] or self._t > self._max_t\n",
        "    self._state = new_state\n",
        "    return self._t, self._state, self._reward, self._done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW3Ul0q-VRE-"
      },
      "source": [
        "## DP Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3ucYXx5NqStY"
      },
      "outputs": [],
      "source": [
        "# This class define the Dynamic Programing agent \n",
        "\n",
        "class DP_agent(object):\n",
        "\n",
        "  # [Action required]\n",
        "  # WARNING: make sure this function can be called by the auto-marking script\n",
        "  def solve(self, env):\n",
        "    \"\"\"\n",
        "    Solve a given Maze environment using Dynamic Programming\n",
        "    input: env {Maze object} -- Maze to solve\n",
        "    output: \n",
        "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
        "      - V {np.array} -- Corresponding value function \n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialisation (can be edited)\n",
        "    action_size = env.get_action_size()\n",
        "    state_size = env.get_state_size()\n",
        "\n",
        "    policy = np.zeros((state_size, action_size)) \n",
        "    V = np.zeros(state_size)\n",
        "\n",
        "    #### \n",
        "    # Add your code here\n",
        "    # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()\n",
        "    ####\n",
        "\n",
        "    def compute_value(transition, reward, action, gamma, v, state):\n",
        "      return np.sum(np.dot(transition[state,:,action], (reward[state,:,action] + gamma * v)))\n",
        "\n",
        "    # policy evaluation, terminate when reaching maximum number of iteration or no value function update\n",
        "    def policy_eval(env, policy, V, max_iter):\n",
        "      is_break = False\n",
        "      for i in range(max_iter):\n",
        "        delta = 0\n",
        "        for state in range(state_size):\n",
        "          v = V[state]\n",
        "          V[state] = compute_value(env.get_T(), env.get_R(), policy[state].argmax(), env.get_gamma(), V, state)\n",
        "          delta = max(delta, np.abs(v - V[state]))\n",
        "        if delta < 0.001:\n",
        "          is_break = True\n",
        "      return V, is_break\n",
        "    \n",
        "    while True:\n",
        "      max_iter = 1000\n",
        "      V, is_break = policy_eval(env, policy, V, max_iter)\n",
        "      policy_stable = True\n",
        "      for state in range(state_size):\n",
        "        b = policy[state].argmax()\n",
        "        transition = env.get_T()\n",
        "        reward = env.get_R()\n",
        "        gamma = env.get_gamma()\n",
        "        policy[state] = np.array([compute_value(transition, reward, a, gamma, V, state) for a in range(action_size)])\n",
        "        policy_stable = policy_stable and (policy[state].argmax() == b)\n",
        "      if max_iter == 1:\n",
        "        if is_break:\n",
        "          break\n",
        "      elif policy_stable:\n",
        "        break\n",
        "    print(policy)\n",
        "    return policy, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14i0zRkdVSqk"
      },
      "source": [
        "## MC agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This class define the Monte-Carlo agent\n",
        "\n",
        "class MC_agent(object):\n",
        "  \n",
        "  # [Action required]\n",
        "  # WARNING: make sure this function can be called by the auto-marking script\n",
        "  def solve(self, env):\n",
        "    \"\"\"\n",
        "    Solve a given Maze environment using Monte Carlo learning\n",
        "    input: env {Maze object} -- Maze to solve\n",
        "    output: \n",
        "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
        "      - values {list of np.array} -- List of successive value functions for each episode \n",
        "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode \n",
        "    \"\"\"\n",
        "\n",
        "    def generate_episode(policy, env):\n",
        "      t, state, _, done = env.reset()\n",
        "      states_actions = []\n",
        "      rewards = []\n",
        "      while not done:\n",
        "        action = np.random.choice(np.arange(0,env.get_action_size()), p=policy[int(state)])\n",
        "        states_actions.append((state, action))\n",
        "        t, state, reward, done = env.step(action)\n",
        "        rewards.append(reward)\n",
        "      return states_actions, np.array(rewards), t\n",
        "            \n",
        "\n",
        "    # Initialisation (can be edited)\n",
        "    action_size = env.get_action_size()\n",
        "    state_size = env.get_state_size()\n",
        "\n",
        "    Q = np.random.rand(state_size, action_size) \n",
        "    V = np.zeros(state_size)\n",
        "    policy = np.zeros((state_size, action_size)) \n",
        "    values = [V]\n",
        "    total_rewards = []\n",
        "\n",
        "    #### \n",
        "    # Add your code here\n",
        "    # WARNING: this agent only has access to env.reset() and env.step()\n",
        "    # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value\n",
        "    ####\n",
        "\n",
        "    returns = [[[] for a in range(action_size)] for s in range(state_size)]\n",
        "    \n",
        "    epsilon = 0.9\n",
        "\n",
        "    for s in range(state_size):\n",
        "      action = np.random.randint(action_size)\n",
        "      for a in range(action_size):\n",
        "        if a == action:\n",
        "          policy[s][a] = 1 - epsilon + (epsilon / action_size)\n",
        "        else:\n",
        "          policy[s][a] = epsilon / action_size\n",
        "\n",
        "    gamma = env.get_gamma()\n",
        "    max_episode = 20000\n",
        "    for episode in range(max_episode):\n",
        "      epsilon = (max_episode - episode) / max_episode\n",
        "      states_actions, rewards, T = generate_episode(policy, env)\n",
        "      total_rewards.append(rewards.sum())\n",
        "      G = 0\n",
        "      for t in range(T-1, -1, -1):\n",
        "        if rewards[t] != -1:\n",
        "          print(states_actions[t])\n",
        "        state = states_actions[t][0]\n",
        "        action = states_actions[t][1]\n",
        "        G = gamma * G + rewards[t]\n",
        "        returns[state][action][0] += G\n",
        "        returns[state][action][1] += 1\n",
        "        Q[state][action] = returns[state][action][0] / returns[state][action][1]\n",
        "        optimal_action = Q[state].argmax()\n",
        "        for a in range(action_size):\n",
        "          if a == optimal_action:\n",
        "            policy[state][a] = 1 - epsilon + (epsilon / action_size)\n",
        "          else:\n",
        "            policy[state][a] = epsilon / action_size\n",
        "\n",
        "    return policy, values, total_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This class define the Monte-Carlo agent\n",
        "\n",
        "class MC_agent(object):\n",
        "  \n",
        "  # [Action required]\n",
        "  # WARNING: make sure this function can be called by the auto-marking script\n",
        "  def solve(self, env):\n",
        "    \"\"\"\n",
        "    Solve a given Maze environment using Monte Carlo learning\n",
        "    input: env {Maze object} -- Maze to solve\n",
        "    output: \n",
        "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
        "      - values {list of np.array} -- List of successive value functions for each episode \n",
        "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode \n",
        "    \"\"\"\n",
        "\n",
        "    def generate_episode(policy, env):\n",
        "      t, state, _, done = env.reset()\n",
        "      states_actions = []\n",
        "      rewards = []\n",
        "      while not done:\n",
        "        action = policy[int(state)]\n",
        "        states_actions.append((state, action))\n",
        "        t, state, reward, done = env.step(action)\n",
        "        rewards.append(reward)\n",
        "      return states_actions, np.array(rewards), t\n",
        "            \n",
        "\n",
        "    # Initialisation (can be edited)\n",
        "    action_size = env.get_action_size()\n",
        "    state_size = env.get_state_size()\n",
        "\n",
        "    Q = np.random.rand(state_size, action_size) \n",
        "    V = np.zeros(state_size)\n",
        "    values = [V]\n",
        "    total_rewards = []\n",
        "\n",
        "\n",
        "    #### \n",
        "    # Add your code here\n",
        "    # WARNING: this agent only has access to env.reset() and env.step()\n",
        "    # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value\n",
        "    ####\n",
        "    \n",
        "    \n",
        "    epsilon = 0.9\n",
        "    C = np.zeros([state_size, action_size]) \n",
        "\n",
        "    def get_soft_policy(epsilon, state_size, action_size):\n",
        "      soft_policy = np.zeros((state_size, action_size))\n",
        "      for s in range(state_size):\n",
        "        action = np.random.randint(action_size)\n",
        "        for a in range(action_size):\n",
        "          if a == action:\n",
        "            soft_policy[s][a] = 1 - epsilon + (epsilon / action_size)\n",
        "          else:\n",
        "            soft_policy[s][a] = epsilon / action_size\n",
        "      return soft_policy\n",
        "    \n",
        "    policy = np.array([Q[s].argmax() for s in range(len(Q))])\n",
        "    \n",
        "    gamma = env.get_gamma()\n",
        "    max_episode = 100000\n",
        "    for _ in range(max_episode):\n",
        "      soft_policy = get_soft_policy(0.9, state_size, action_size)\n",
        "      states_actions, rewards, T = generate_episode(policy, env)\n",
        "      G = 0\n",
        "      W = 1\n",
        "      for t in range(T-1, -1, -1):\n",
        "        state = states_actions[t][0]\n",
        "        action = states_actions[t][1]\n",
        "        G = gamma * G + rewards[t]\n",
        "        C[state][action] += W\n",
        "        Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
        "        policy[state] = Q[state].argmax()\n",
        "        if action != policy[state]:\n",
        "          break\n",
        "        W = W / soft_policy[state][action]\n",
        "    \n",
        "\n",
        "    temp_policy = policy\n",
        "    policy = np.zeros((state_size, action_size))\n",
        "    for s in range(state_size):\n",
        "      for a in range(action_size):\n",
        "        if temp_policy[s] == a:\n",
        "          policy[s][a] = 1\n",
        "        else:\n",
        "          policy[s][a] = 0\n",
        "\n",
        "    return policy, values, total_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMkZKrh6VUgw"
      },
      "source": [
        "## TD agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "_Xyko9SvrGbE"
      },
      "outputs": [],
      "source": [
        "# This class define the Temporal-Difference agent\n",
        "\n",
        "class TD_agent(object):\n",
        "\n",
        "  # [Action required]\n",
        "  # WARNING: make sure this function can be called by the auto-marking script\n",
        "  def solve(self, env):\n",
        "    \"\"\"\n",
        "    Solve a given Maze environment using Temporal Difference learning\n",
        "    input: env {Maze object} -- Maze to solve\n",
        "    output: \n",
        "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
        "      - values {list of np.array} -- List of successive value functions for each episode \n",
        "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode \n",
        "    \"\"\"\n",
        "\n",
        "    # Initialisation (can be edited)\n",
        "    action_size = env.get_action_size()\n",
        "    state_size = env.get_state_size()\n",
        "\n",
        "    Q = np.random.rand(state_size, action_size) \n",
        "    V = np.zeros(state_size)\n",
        "    policy = np.zeros((state_size, action_size)) \n",
        "    values = [V]\n",
        "    total_rewards = []\n",
        "\n",
        "    #### \n",
        "    # Add your code here\n",
        "    # WARNING: this agent only has access to env.reset() and env.step()\n",
        "    # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value\n",
        "    ####\n",
        "    \n",
        "    return policy, values, total_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzSzRSO6VWVD"
      },
      "source": [
        "## Example main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eyeJfvwXp3ta",
        "outputId": "229af227-7973-4819-dc05-ff8219987805",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating the Maze:\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn8AAAMtCAYAAADufxezAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAg8ElEQVR4nO3de4yU9bnA8WdZygBmWQXDyoZFMfFEBRUENYJajJYTVFrTVusdtW284AVJLOCtagsbbUtJJGAwjaX1oJykXoiprRtvSK2RqxrbiFYiW5FQG7KLWFdY5vxxdNMVqC47wwjP55O8f8w7787vIaPJN7+5VRWLxWIAAJBCj0oPAADA3iP+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQSM9KD/B5O3bsiA0bNkRNTU1UVVVVehwAgH1CsViMLVu2RH19ffTosfv9va9c/G3YsCEaGhoqPQYAwD6pubk5Bg8evNv7v3LxV1NTExERx5x/e1R/rXeFpwEA2De0b/s4Xv/fn3S01O585eLvs5d6q7/WO6p7iT8AgK74orfN+cAHAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImULf7mzZsXQ4cOjd69e8eoUaPixRdfLNdSAAB8SWWJv8WLF8eUKVPi1ltvjdWrV8epp54aEyZMiPXr15djOQAAvqSyxN/s2bPj+9//fvzgBz+Io446KubMmRMNDQ0xf/78ciwHAMCXVPL4++STT2LlypUxfvz4TufHjx8fL7300k7Xt7W1RWtra6cDAIDyKHn8ffDBB9He3h51dXWdztfV1cXGjRt3ur6xsTFqa2s7joaGhlKPBADAp8r2gY+qqqpOt4vF4k7nIiJmzJgRLS0tHUdzc3O5RgIASK9nqR/w4IMPjurq6p12+TZt2rTTbmBERKFQiEKhUOoxAADYhZLv/PXq1StGjRoVTU1Nnc43NTXFmDFjSr0cAABdUPKdv4iIqVOnxqWXXhqjR4+Ok08+ORYsWBDr16+Pq6++uhzLAQDwJZUl/r73ve/FP//5z7j77rvj/fffj+HDh8fvf//7OPTQQ8uxHAAAX1JZ4i8i4tprr41rr722XA8PAMAe8Nu+AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIpKpYLBYrPcS/a21tjdra2ti89vDoV6NNga+WE2+5ptIjUCavzJpf6RGgW1q37IiD/uudaGlpiX79+u32OnUFAJCI+AMASET8AQAkIv4AABIRfwAAiYg/AIBExB8AQCLiDwAgEfEHAJCI+AMASET8AQAkIv4AABIRfwAAiYg/AIBExB8AQCLiDwAgEfEHAJCI+AMASET8AQAkIv4AABIRfwAAiYg/AIBExB8AQCLiDwAgEfEHAJCI+AMASET8AQAkIv4AABIRfwAAiYg/AIBExB8AQCLiDwAgEfEHAJCI+AMASET8AQAkIv4AABIRfwAAiYg/AIBExB8AQCLiDwAgEfEHAJCI+AMASET8AQAkIv4AABIRfwAAiYg/AIBExB8AQCLiDwAgEfEHAJCI+AMASET8AQAkIv4AABIRfwAAiYg/AIBExB8AQCLiDwAgEfEHAJCI+AMASKRnpQfYnbP/elb0PKBQ6THK7rlhT1R6BOi2E2+5ptIjAF2Q7f/Zg37950qPsFdsL26LiHe+8Do7fwAAiYg/AIBExB8AQCLiDwAgEfEHAJCI+AMASET8AQAkIv4AABIRfwAAiYg/AIBExB8AQCLiDwAgEfEHAJCI+AMASET8AQAkIv4AABIRfwAAiYg/AIBExB8AQCLiDwAgEfEHAJCI+AMASET8AQAkIv4AABIRfwAAiYg/AIBESh5/jY2NccIJJ0RNTU0MHDgwzj333HjzzTdLvQwAAHug5PH3wgsvxOTJk+Pll1+Opqam2L59e4wfPz62bt1a6qUAAOiinqV+wD/84Q+dbj/44IMxcODAWLlyZZx22mk7Xd/W1hZtbW0dt1tbW0s9EgAAnyr7e/5aWloiIqJ///67vL+xsTFqa2s7joaGhnKPBACQVlnjr1gsxtSpU+OUU06J4cOH7/KaGTNmREtLS8fR3NxczpEAAFIr+cu+/+66666L1157LZYtW7bbawqFQhQKhXKOAQDAp8oWf9dff30sWbIkli5dGoMHDy7XMgAAdEHJ469YLMb1118fjz32WDz//PMxdOjQUi8BAMAeKnn8TZ48ORYtWhRPPPFE1NTUxMaNGyMiora2Nvr06VPq5QAA6IKSf+Bj/vz50dLSEuPGjYtBgwZ1HIsXLy71UgAAdFFZXvYFAOCryW/7AgAkIv4AABIRfwAAiYg/AIBExB8AQCLiDwAgEfEHAJCI+AMASET8AQAkIv4AABIRfwAAiYg/AIBExB8AQCLiDwAgEfEHAJCI+AMASET8AQAkIv4AABIRfwAAiYg/AIBExB8AQCLiDwAgkZ6VHmB3PvrfQVHdq3elxyi/WZUeAAD2b5svP7nSI+wV7Z98HPE/T3zhdXb+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAIn0rPQA5HLiLddUegQASM3OHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAImWPv8bGxqiqqoopU6aUeykAAL5AWeNv+fLlsWDBgjj22GPLuQwAAF9S2eLvww8/jIsvvjgeeOCBOOigg8q1DAAAXVC2+Js8eXKcffbZceaZZ/7H69ra2qK1tbXTAQBAefQsx4M+8sgjsWrVqli+fPkXXtvY2Bh33XVXOcYAAOBzSr7z19zcHDfeeGM89NBD0bt37y+8fsaMGdHS0tJxNDc3l3okAAA+VfKdv5UrV8amTZti1KhRHefa29tj6dKlMXfu3Ghra4vq6uqO+wqFQhQKhVKPAQDALpQ8/s4444x4/fXXO5274oor4sgjj4xp06Z1Cj8AAPauksdfTU1NDB8+vNO5Aw44IAYMGLDTeQAA9i6/8AEAkEhZPu37ec8///zeWAYAgC9g5w8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIJGelR4guxNvuabSIwBd8Mqs+ZUeAaBb7PwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEilL/L333ntxySWXxIABA6Jv374xYsSIWLlyZTmWAgCgC3qW+gE3b94cY8eOjdNPPz2eeuqpGDhwYPztb3+LAw88sNRLAQDQRSWPv3vuuScaGhriwQcf7Dh32GGH7fb6tra2aGtr67jd2tpa6pEAAPhUyV/2XbJkSYwePTrOO++8GDhwYIwcOTIeeOCB3V7f2NgYtbW1HUdDQ0OpRwIA4FMlj7933nkn5s+fH0cccUT88Y9/jKuvvjpuuOGG+M1vfrPL62fMmBEtLS0dR3Nzc6lHAgDgUyV/2XfHjh0xevTomDVrVkREjBw5Mt54442YP39+XHbZZTtdXygUolAolHoMAAB2oeQ7f4MGDYqjjz6607mjjjoq1q9fX+qlAADoopLH39ixY+PNN9/sdG7t2rVx6KGHlnopAAC6qOTxd9NNN8XLL78cs2bNirfffjsWLVoUCxYsiMmTJ5d6KQAAuqjk8XfCCSfEY489Fg8//HAMHz48fvKTn8ScOXPi4osvLvVSAAB0Uck/8BERcc4558Q555xTjocGAKAb/LYvAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJNKz0gMA7EtOvOWaSo8A0C12/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgER6VnqA3Xnux7+KfjX7f5ueeMs1lR4BAEhk/68rAAA6iD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJlDz+tm/fHrfddlsMHTo0+vTpE4cffnjcfffdsWPHjlIvBQBAF/Us9QPec889cf/998fChQtj2LBhsWLFirjiiiuitrY2brzxxlIvBwBAF5Q8/v785z/Ht771rTj77LMjIuKwww6Lhx9+OFasWFHqpQAA6KKSv+x7yimnxDPPPBNr166NiIhXX301li1bFmedddYur29ra4vW1tZOBwAA5VHynb9p06ZFS0tLHHnkkVFdXR3t7e0xc+bMuPDCC3d5fWNjY9x1112lHgMAgF0o+c7f4sWL46GHHopFixbFqlWrYuHChfHzn/88Fi5cuMvrZ8yYES0tLR1Hc3NzqUcCAOBTJd/5u/nmm2P69OlxwQUXRETEMcccE++++240NjbGpEmTdrq+UChEoVAo9RgAAOxCyXf+Pvroo+jRo/PDVldX+6oXAICvgJLv/E2cODFmzpwZQ4YMiWHDhsXq1atj9uzZceWVV5Z6KQAAuqjk8XfffffF7bffHtdee21s2rQp6uvr46qrroo77rij1EsBANBFJY+/mpqamDNnTsyZM6fUDw0AQDf5bV8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACRSVSwWi5Ue4t+1trZGbW1tbF57ePSr0absu0685ZpKjwDd9sqs+ZUeAfiSWrfsiIP+651oaWmJfv367fY6dQUAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIpGelB8juv+tHVHqEvWrz5SdXegSgC0685ZpKjwB8Se2ffBwRt37hdXb+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAAS6XL8LV26NCZOnBj19fVRVVUVjz/+eKf7i8Vi3HnnnVFfXx99+vSJcePGxRtvvFGqeQEA6IYux9/WrVvjuOOOi7lz5+7y/nvvvTdmz54dc+fOjeXLl8chhxwS3/jGN2LLli3dHhYAgO7p2dU/mDBhQkyYMGGX9xWLxZgzZ07ceuut8e1vfzsiIhYuXBh1dXWxaNGiuOqqq7o3LQAA3VLS9/ytW7cuNm7cGOPHj+84VygU4utf/3q89NJLu/ybtra2aG1t7XQAAFAeJY2/jRs3RkREXV1dp/N1dXUd931eY2Nj1NbWdhwNDQ2lHAkAgH9Tlk/7VlVVdbpdLBZ3OveZGTNmREtLS8fR3NxcjpEAAIg9eM/ff3LIIYdExP/vAA4aNKjj/KZNm3baDfxMoVCIQqFQyjEAANiNku78DR06NA455JBoamrqOPfJJ5/ECy+8EGPGjCnlUgAA7IEu7/x9+OGH8fbbb3fcXrduXaxZsyb69+8fQ4YMiSlTpsSsWbPiiCOOiCOOOCJmzZoVffv2jYsuuqikgwMA0HVdjr8VK1bE6aef3nF76tSpERExadKk+PWvfx0/+tGP4l//+ldce+21sXnz5jjppJPi6aefjpqamtJNDQDAHqkqFovFSg/x71pbW6O2tjY2rz08+tXs/78+99/1Iyo9wl61+fKTKz0CAOyX2j/5ONb8z63R0tIS/fr12+11+39dAQDQQfwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIj0rPcDunH7X96O6V+9Kj1F+l1d6AAAgEzt/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIj0rPQAAQDkd9Os/V3qEvWJ7cduXus7OHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIl2Ov6VLl8bEiROjvr4+qqqq4vHHH++4b9u2bTFt2rQ45phj4oADDoj6+vq47LLLYsOGDaWcGQCAPdTl+Nu6dWscd9xxMXfu3J3u++ijj2LVqlVx++23x6pVq+LRRx+NtWvXxje/+c2SDAsAQPf07OofTJgwISZMmLDL+2pra6OpqanTufvuuy9OPPHEWL9+fQwZMmTPpgQAoCS6HH9d1dLSElVVVXHggQfu8v62trZoa2vruN3a2lrukQAA0irrBz4+/vjjmD59elx00UXRr1+/XV7T2NgYtbW1HUdDQ0M5RwIASK1s8bdt27a44IILYseOHTFv3rzdXjdjxoxoaWnpOJqbm8s1EgBAemV52Xfbtm1x/vnnx7p16+LZZ5/d7a5fREShUIhCoVCOMQAA+JySx99n4ffWW2/Fc889FwMGDCj1EgAA7KEux9+HH34Yb7/9dsftdevWxZo1a6J///5RX18f3/3ud2PVqlXx5JNPRnt7e2zcuDEiIvr37x+9evUq3eQAAHRZl+NvxYoVcfrpp3fcnjp1akRETJo0Ke68885YsmRJRESMGDGi098999xzMW7cuD2fFACAbuty/I0bNy6KxeJu7/9P9wEAUFl+2xcAIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAASEX8AAImIPwCARMQfAEAi4g8AIBHxBwCQiPgDAEhE/AEAJCL+AAAS6VnpAT6vWCxGRET7to8rPAkAsD/YXtxW6RH2iu3x///Oz1pqd6qKX3TFXvb3v/89GhoaKj0GAMA+qbm5OQYPHrzb+79y8bdjx47YsGFD1NTURFVV1V5bt7W1NRoaGqK5uTn69eu319al/Dy3+yfP6/7J87r/8tyWX7FYjC1btkR9fX306LH7d/Z95V727dGjx3+s1XLr16+f/yj3U57b/ZPndf/ked1/eW7Lq7a29guv8YEPAIBExB8AQCLi71OFQiF+/OMfR6FQqPQolJjndv/ked0/eV73X57br46v3Ac+AAAoHzt/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4+9S8efNi6NCh0bt37xg1alS8+OKLlR6JbmhsbIwTTjghampqYuDAgXHuuefGm2++WemxKLHGxsaoqqqKKVOmVHoUSuC9996LSy65JAYMGBB9+/aNESNGxMqVKys9Ft2wffv2uO2222Lo0KHRp0+fOPzww+Puu++OHTt2VHq01MRfRCxevDimTJkSt956a6xevTpOPfXUmDBhQqxfv77So7GHXnjhhZg8eXK8/PLL0dTUFNu3b4/x48fH1q1bKz0aJbJ8+fJYsGBBHHvssZUehRLYvHlzjB07Nr72ta/FU089FX/5y1/iF7/4RRx44IGVHo1uuOeee+L++++PuXPnxl//+te4995742c/+1ncd999lR4tNd/zFxEnnXRSHH/88TF//vyOc0cddVSce+650djYWMHJKJV//OMfMXDgwHjhhRfitNNOq/Q4dNOHH34Yxx9/fMybNy9++tOfxogRI2LOnDmVHotumD59evzpT3/yqst+5pxzzom6urr41a9+1XHuO9/5TvTt2zd++9vfVnCy3NLv/H3yySexcuXKGD9+fKfz48ePj5deeqlCU1FqLS0tERHRv3//Ck9CKUyePDnOPvvsOPPMMys9CiWyZMmSGD16dJx33nkxcODAGDlyZDzwwAOVHotuOuWUU+KZZ56JtWvXRkTEq6++GsuWLYuzzjqrwpPl1rPSA1TaBx98EO3t7VFXV9fpfF1dXWzcuLFCU1FKxWIxpk6dGqecckoMHz680uPQTY888kisWrUqli9fXulRKKF33nkn5s+fH1OnTo1bbrklXnnllbjhhhuiUCjEZZddVunx2EPTpk2LlpaWOPLII6O6ujra29tj5syZceGFF1Z6tNTSx99nqqqqOt0uFos7nWPfdN1118Vrr70Wy5Ytq/QodFNzc3PceOON8fTTT0fv3r0rPQ4ltGPHjhg9enTMmjUrIiJGjhwZb7zxRsyfP1/87cMWL14cDz30UCxatCiGDRsWa9asiSlTpkR9fX1MmjSp0uOllT7+Dj744Kiurt5pl2/Tpk077Qay77n++utjyZIlsXTp0hg8eHClx6GbVq5cGZs2bYpRo0Z1nGtvb4+lS5fG3Llzo62tLaqrqys4IXtq0KBBcfTRR3c6d9RRR8Xvfve7Ck1EKdx8880xffr0uOCCCyIi4phjjol33303GhsbxV8FpX/PX69evWLUqFHR1NTU6XxTU1OMGTOmQlPRXcViMa677rp49NFH49lnn42hQ4dWeiRK4IwzzojXX3891qxZ03GMHj06Lr744lizZo3w24eNHTt2p69jWrt2bRx66KEVmohS+Oijj6JHj86pUV1d7ateKiz9zl9ExNSpU+PSSy+N0aNHx8knnxwLFiyI9evXx9VXX13p0dhDkydPjkWLFsUTTzwRNTU1HTu7tbW10adPnwpPx56qqanZ6X2bBxxwQAwYMMD7OfdxN910U4wZMyZmzZoV559/frzyyiuxYMGCWLBgQaVHoxsmTpwYM2fOjCFDhsSwYcNi9erVMXv27LjyyisrPVpqvurlU/PmzYt777033n///Rg+fHj88pe/9JUg+7DdvV/zwQcfjMsvv3zvDkNZjRs3zle97CeefPLJmDFjRrz11lsxdOjQmDp1avzwhz+s9Fh0w5YtW+L222+Pxx57LDZt2hT19fVx4YUXxh133BG9evWq9HhpiT8AgETSv+cPACAT8QcAkIj4AwBIRPwBACQi/gAAEhF/AACJiD8AgETEHwBAIuIPACAR8QcAkIj4AwBI5P8AJ32grvdtw3cAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1500x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(18, 0)\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Andy Fu\\Desktop\\rl\\coursework_1_maze.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m### Question 1: Dynamic programming\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# dp_agent = DP_agent()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m### Question 2: Monte-Carlo learning\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m mc_agent \u001b[39m=\u001b[39m MC_agent()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m mc_policy, mc_values, total_rewards \u001b[39m=\u001b[39m mc_agent\u001b[39m.\u001b[39;49msolve(maze)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mResults of the MC agent:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m maze\u001b[39m.\u001b[39mget_graphics()\u001b[39m.\u001b[39mdraw_policy(mc_policy)\n",
            "\u001b[1;32mc:\\Users\\Andy Fu\\Desktop\\rl\\coursework_1_maze.ipynb Cell 16\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m action \u001b[39m=\u001b[39m states_actions[t][\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m G \u001b[39m=\u001b[39m gamma \u001b[39m*\u001b[39m G \u001b[39m+\u001b[39m rewards[t]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m returns[state][action][\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m G\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m returns[state][action][\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy%20Fu/Desktop/rl/coursework_1_maze.ipynb#X21sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m Q[state][action] \u001b[39m=\u001b[39m returns[state][action][\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m returns[state][action][\u001b[39m1\u001b[39m]\n",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# Example main (can be edited)\n",
        "\n",
        "### Question 0: Defining the environment\n",
        "\n",
        "print(\"Creating the Maze:\\n\")\n",
        "maze = Maze()\n",
        "\n",
        "\n",
        "### Question 1: Dynamic programming\n",
        "\n",
        "# dp_agent = DP_agent()\n",
        "# dp_policy, dp_value = dp_agent.solve(maze)\n",
        "\n",
        "# print(\"Results of the DP agent:\\n\")\n",
        "# maze.get_graphics().draw_policy(dp_policy)\n",
        "# maze.get_graphics().draw_value(dp_value)\n",
        "\n",
        "\n",
        "### Question 2: Monte-Carlo learning\n",
        "\n",
        "mc_agent = MC_agent()\n",
        "mc_policy, mc_values, total_rewards = mc_agent.solve(maze)\n",
        "\n",
        "print(\"Results of the MC agent:\\n\")\n",
        "maze.get_graphics().draw_policy(mc_policy)\n",
        "maze.get_graphics().draw_value(mc_values[-1])\n",
        "\n",
        "\n",
        "### Question 3: Temporal-Difference learning\n",
        "\n",
        "# td_agent = TD_agent()\n",
        "# td_policy, td_values, total_rewards = td_agent.solve(maze)\n",
        "\n",
        "# print(\"Results of the TD agent:\\n\")\n",
        "# maze.get_graphics().draw_policy(td_policy)\n",
        "# maze.get_graphics().draw_value(td_values[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "lbY8DCqoVJlw",
        "DW3Ul0q-VRE-",
        "14i0zRkdVSqk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
